{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept, nonlinear_old\n",
    "import igraph as ig\n",
    "# import lingam\n",
    "# from lingam.utils import make_prior_knowledge, make_dot\n",
    "import ray\n",
    "import pickle as pk\n",
    "from scipy.special import expit as sigmoid\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.23.3', '1.4.2']\n"
     ]
    }
   ],
   "source": [
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change_0 ===========================================================================================\n",
    "\n",
    "# df_1 = pd.read_csv('datasets/adult_data.csv', header=None)\n",
    "# df_2 = pd.read_csv('datasets/adult_test.csv', header=None)\n",
    "# df_3 = pd.concat([df_1, df_2])\n",
    "# for col in list(df_3.columns):\n",
    "#     df_3 = df_3[df_3[col] != ' ?'] \n",
    "\n",
    "# list_col = list(df_3.columns)\n",
    "# list_remove = [2,4,10,11] ## fnlwgt, education-num, capital-gain, capital-loss\n",
    "# list_col_valid = []\n",
    "# for col in list_col:\n",
    "#     if col not in list_remove:\n",
    "#         list_col_valid.append(col)\n",
    "# print(list_col_valid)\n",
    "\n",
    "# df_4 = df_3[list_col_valid]\n",
    "# df_4 = df_4.replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'})\n",
    "# print(df_4.shape)\n",
    "\n",
    "# df_4.to_csv('datasets/adult_processed.csv', index=False, header=None) \n",
    "\n",
    "# change_0 ==========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 11)\n",
      "(11, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>native country</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>hour/week</th>\n",
       "      <th>workingclass</th>\n",
       "      <th>maritial</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      race  age   native country  sex  education  hour/week  workingclass  \\\n",
       "race     0     0               0    0          1          1             0   \n",
       "age      0     0               0    0          1          1             1   \n",
       "\n",
       "      maritial  occupation  relationship  income  \n",
       "race         1           1             0       1  \n",
       "age          1           1             1       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change_1 ============================================================================================\n",
    "## data and causal graph\n",
    "\n",
    "df_x = pd.read_csv('datasets/adult_processed.csv', header=None)\n",
    "df_cg = pd.read_excel(open('datasets/adult.xlsx', 'rb'), index_col=0)\n",
    "\n",
    "print(df_x.shape)\n",
    "df_x.head(2)\n",
    "\n",
    "print(df_cg.shape)\n",
    "df_cg.head(2)\n",
    "## change_1 ============================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### experiment\n",
    "\n",
    "## functions and classes \n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(data_x, data_cg, should_std, trial_no):\n",
    "        \n",
    "    ## 1\n",
    "    np.random.seed(123+trial_no) \n",
    "    ut.set_random_seed(123+trial_no) \n",
    "\n",
    "    ## 2\n",
    "    ## change_2 ==========================================================================================    \n",
    "\n",
    "    ## 0. age: continuous.\n",
    "    ## 1. workclass: -\n",
    "    ## 2. education: -\n",
    "    ## 3. marital-status: -\n",
    "    ## 4. occupation: -\n",
    "    ## 5. relationship: -\n",
    "    ## 6. race: -\n",
    "    ## 7. sex: -\n",
    "    ## 8. hours-per-week: continuous.\n",
    "    ## 9. native-country: -\n",
    "    ## 10. income: -     \n",
    "    \n",
    "    list_index_continuous = [0, 8] ## age, hours per week\n",
    "    Xcon, B_true = df_x.values, df_cg.values\n",
    "    # ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    # obj_1 = ohe.fit(Xcon)\n",
    "    # # print(obj_1.categories_)\n",
    "    # Xflat = obj_1.transform(Xcon).toarray()\n",
    "    Xflat = None\n",
    "    for i in range(Xcon.shape[1]):\n",
    "        Xlocal = Xcon[:, i].reshape(-1, 1)    \n",
    "        if i in list_index_continuous:\n",
    "            Xlocalflat = Xlocal\n",
    "        else:\n",
    "            Xlocalflat = OneHotEncoder(handle_unknown='ignore').fit(Xlocal).transform(Xlocal).toarray()\n",
    "        if Xflat is None:\n",
    "            Xflat = Xlocalflat\n",
    "        else:\n",
    "            Xflat = np.hstack((Xflat, Xlocalflat))\n",
    "        # print(i, Xlocal.shape, Xlocalflat.shape)\n",
    "    \n",
    "    \n",
    "    n, d = Xcon.shape\n",
    "    s0 = sum(sum(B_true))   \n",
    "    # concepts = [2] * d\n",
    "    concepts = []\n",
    "    for i in list(df_x.columns):\n",
    "        if i in list_index_continuous:\n",
    "            cval = 1\n",
    "        else:\n",
    "            cval = len(list(df_x[i].unique()))\n",
    "        concepts.append(cval)\n",
    "    print('concepts ======> ', concepts)\n",
    "    dcon, dflat = len(concepts), sum(concepts)\n",
    "\n",
    "    print(n, d, s0, concepts, dcon, dflat, Xcon.shape, B_true.shape, Xflat.shape)\n",
    "\n",
    "    ## 3\n",
    "    if should_std:\n",
    "        # scalerCon = StandardScaler().fit(Xcon)\n",
    "        # Xcon = scalerCon.transform(Xcon) ## this includes string values, so can't transform    \n",
    "        scalerFlat = StandardScaler().fit(Xflat)\n",
    "        Xflat = scalerFlat.transform(Xflat)    \n",
    "    # Xcon, Xflat = Xcon.astype('float32'), Xflat.astype('float32')\n",
    "    Xflat = Xflat.astype('float32')\n",
    "    ## change_2 ==========================================================================================    \n",
    "        \n",
    "\n",
    "    ## 4\n",
    "    mask = np.ones((dcon, dcon)) * np.nan\n",
    "    print(concepts, dcon, dflat)\n",
    "    assert len(concepts) == dcon \n",
    "    assert sum(concepts) == dflat\n",
    "    assert Xcon.shape[1] == dcon        \n",
    "    assert Xflat.shape[1] == dflat    \n",
    "\n",
    "    ## initializing model and running the optimizationportion_parent\n",
    "    try:\n",
    "        metainfo = {}\n",
    "        metainfo['dflat'] = dflat\n",
    "        metainfo['dcon'] = dcon\n",
    "        metainfo['concepts'] = concepts                            \n",
    "        model = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dflat, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=0.2, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo\n",
    "        )\n",
    "        W_notears, res = nonlinear_concept.notears_nonlinear(\n",
    "            model, Xflat, lambda1=0.001, lambda2=0.001,\n",
    "            h_tol=1e-4, rho_max=1e+8\n",
    "        ) ## lambda1=0.01, lambda2=0.01, h_tol=1e-8, rho_max=1e+16\n",
    "        # assert ut.is_dag(W_notears)\n",
    "        # np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_notears != 0)\n",
    "        print('nCon: ', acc)\n",
    "        print(W_notears)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, nCon ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            should_std, trial_no, \n",
    "            acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']\n",
    "        )\n",
    "        file1.writelines(s1)\n",
    "        file1.close()    \n",
    "        #\n",
    "    except Exception as e:\n",
    "        print('========================================', e)\n",
    "        acc = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    ## initializing model and running the optimizaportion_parenttion\n",
    "    def conv_flat_to_con(A, concepts):\n",
    "\n",
    "        ##\n",
    "        A = np.abs(A) ## in the optimization this works on square matrix, so there we don't need to abs it\n",
    "        dflat = sum(concepts)\n",
    "        dcon = len(concepts)\n",
    "        Arow = np.zeros((dcon,dflat))\n",
    "        Ad = np.zeros((dcon,dcon))\n",
    "        end_concept = np.cumsum(concepts)\n",
    "\n",
    "        ##\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Arow[i,:] = (A[start_i:end_i,:].sum(axis=0))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Ad[:,i] = (Arow[:,start_i:end_i].sum(axis=1))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "\n",
    "        ##\n",
    "        new_adj_mat = np.zeros((dcon,dcon))\n",
    "        for i in range(dcon):\n",
    "            for j in range(dcon):\n",
    "                if Ad[i][j] != 0:\n",
    "                    new_adj_mat[i][j] = 1\n",
    "\n",
    "        return new_adj_mat\n",
    "\n",
    "    try:\n",
    "        model3 = nonlinear_old.NotearsMLP(dims=[dflat, 10, 1], bias=True)\n",
    "        W_notears3 = nonlinear_old.notears_nonlinear(\n",
    "            model3, Xflat, lambda1=0.001, lambda2=0.001, w_threshold=0.2,\n",
    "            h_tol=1e-4, rho_max=1e+8\n",
    "        ) ## lambda1=0.01, lambda2=0.01, w_threshold=0.3, h_tol=1e-8, rho_max=1e+16\n",
    "        W_notears3 = conv_flat_to_con(W_notears3, concepts)\n",
    "        # assert ut.is_dag(W_notears3)\n",
    "        # np.savetxt('outputs/W_notears3.csv', W_notears3, delimiter=',')\n",
    "        acc3 = ut.count_accuracy(B_true, W_notears3 != 0)\n",
    "        print('nRegFlat', acc3)\n",
    "        print(W_notears3)        \n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, nRegFlat ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            should_std, trial_no, \n",
    "            acc3['fdr'], acc3['tpr'], acc3['fpr'], acc3['shd'], acc3['nnz']\n",
    "        )                            \n",
    "        file1.writelines(s1)\n",
    "        file1.close()\n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc3 = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    #################################################\n",
    " \n",
    "    \n",
    "    return [\n",
    "        (acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']), \n",
    "        (acc3['fdr'], acc3['tpr'], acc3['fpr'], acc3['shd'], acc3['nnz']),        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 18:20:17,310\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(get_result pid=6148)\u001b[0m concepts ======>  [1, 7, 16, 7, 14, 6, 5, 2, 1, 41, 2]\n",
      "\u001b[2m\u001b[36m(get_result pid=6148)\u001b[0m 45222 11 40 [1, 7, 16, 7, 14, 6, 5, 2, 1, 41, 2] 11 102 (45222, 11) (11, 11) (45222, 102)\n",
      "\u001b[2m\u001b[36m(get_result pid=17516)\u001b[0m concepts ======>  [1, 7, 16, 7, 14, 6, 5, 2, 1, 41, 2]\n",
      "\u001b[2m\u001b[36m(get_result pid=17516)\u001b[0m 45222 11 40 [1, 7, 16, 7, 14, 6, 5, 2, 1, 41, 2] 11 102 (45222, 11) (11, 11) (45222, 102)\n",
      "\u001b[2m\u001b[36m(get_result pid=6148)\u001b[0m [1, 7, 16, 7, 14, 6, 5, 2, 1, 41, 2] 11 102\n",
      "\u001b[2m\u001b[36m(get_result pid=6148)\u001b[0m -----iteration no:  0\n",
      "\u001b[2m\u001b[36m(get_result pid=17516)\u001b[0m [1, 7, 16, 7, 14, 6, 5, 2, 1, 41, 2] 11 102\n",
      "\u001b[2m\u001b[36m(get_result pid=17516)\u001b[0m -----iteration no:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     result_id \u001b[38;5;241m=\u001b[39m get_result\u001b[38;5;241m.\u001b[39mremote(\n\u001b[0;32m     16\u001b[0m         df_x, df_cg, should_std, trial_no\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     list_result_id\u001b[38;5;241m.\u001b[39mappend(result_id)\n\u001b[1;32m---> 19\u001b[0m list_result \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(list_result_id)\n\u001b[0;32m     21\u001b[0m d_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_trials):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_code_notears_concept\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_code_notears_concept\\lib\\site-packages\\ray\\_private\\worker.py:2269\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m     )\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[1;32m-> 2269\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_code_notears_concept\\lib\\site-packages\\ray\\_private\\worker.py:669\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[1;34m(self, object_refs, timeout)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    668\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 669\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:1211\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:173\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    ## variables\n",
    "    list_should_std = [False, True]\n",
    "    n_trials = 2\n",
    "    \n",
    "    ## variables            \n",
    "    ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=56) ## detects automatically: num_cpus=64\n",
    "\n",
    "    ## experiments\n",
    "    for should_std in list_should_std:\n",
    "        list_result_id = []\n",
    "        for trial_no in range(n_trials):\n",
    "            result_id = get_result.remote(\n",
    "                df_x, df_cg, should_std, trial_no\n",
    "            )\n",
    "            list_result_id.append(result_id)\n",
    "        list_result = ray.get(list_result_id)\n",
    "\n",
    "        d_result = {}\n",
    "        for trial_no in range(n_trials):\n",
    "            d_result[(should_std, trial_no, 'nCon')] = list_result[trial_no][0]\n",
    "            d_result[(should_std, trial_no, 'nRegFlat')] = list_result[trial_no][1]                                \n",
    "\n",
    "        with open(\n",
    "            'datasets/d_result_' + str(should_std) + '.pickle', 'wb'\n",
    "        ) as handle: \n",
    "            pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
