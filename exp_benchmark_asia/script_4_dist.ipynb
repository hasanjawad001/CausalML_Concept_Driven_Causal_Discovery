{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept, nonlinear_old\n",
    "import igraph as ig\n",
    "# import lingam\n",
    "# from lingam.utils import make_prior_knowledge, make_dot\n",
    "import ray\n",
    "import pickle as pk\n",
    "from scipy.special import expit as sigmoid\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "## data and causal graph\n",
    "\n",
    "df_x = pd.read_excel(open('datasets/asia.xlsx', 'rb'), sheet_name='0', index_col=None)\n",
    "df_x = df_x.replace(['no', 'yes'], [0, 1])\n",
    "df_cg = pd.read_excel(open('datasets/asia.xlsx', 'rb'), sheet_name='1', index_col=0)\n",
    "\n",
    "print(df_x.shape)\n",
    "df_x.head(2)\n",
    "\n",
    "print(df_cg.shape)\n",
    "df_cg.head(2)\n",
    "\n",
    "\n",
    "\n",
    "#### experiment\n",
    "\n",
    "## functions and classes \n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(data_x, data_cg, should_std, trial_no):\n",
    "    ## 1\n",
    "    np.random.seed(123+trial_no) \n",
    "    ut.set_random_seed(123+trial_no) \n",
    "\n",
    "    ## 2\n",
    "    Xcon, B_true = df_x.values, df_cg.values\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    obj_1 = ohe.fit(Xcon)\n",
    "    # print(obj_1.categories_)\n",
    "    Xflat = obj_1.transform(Xcon).toarray()\n",
    "\n",
    "    n, d = Xcon.shape\n",
    "    s0 = sum(sum(B_true))   \n",
    "    concepts = [2] * d\n",
    "    dcon, dflat = len(concepts), sum(concepts)\n",
    "\n",
    "    print(n, d, s0, concepts, dcon, dflat, Xcon.shape, B_true.shape, Xflat.shape)\n",
    "\n",
    "    ## 3\n",
    "    if should_std:\n",
    "        scalerCon = StandardScaler().fit(Xcon)\n",
    "        Xcon = scalerCon.transform(Xcon)    \n",
    "        scalerFlat = StandardScaler().fit(Xflat)\n",
    "        Xflat = scalerFlat.transform(Xflat)    \n",
    "    Xcon, Xflat = Xcon.astype('float32'), Xflat.astype('float32')\n",
    "        \n",
    "\n",
    "    ## 4\n",
    "    mask = np.ones((dcon, dcon)) * np.nan\n",
    "    print(concepts, dcon, dflat)\n",
    "    assert len(concepts) == dcon \n",
    "    assert sum(concepts) == dflat\n",
    "    assert Xcon.shape[1] == dcon        \n",
    "    assert Xflat.shape[1] == dflat    \n",
    "\n",
    "    ## initializing model and running the optimizationportion_parent\n",
    "    try:\n",
    "        metainfo = {}\n",
    "        metainfo['dflat'] = dflat\n",
    "        metainfo['dcon'] = dcon\n",
    "        metainfo['concepts'] = concepts                            \n",
    "        model = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dflat, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=0.2, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo\n",
    "        )\n",
    "        W_notears, res = nonlinear_concept.notears_nonlinear(\n",
    "            model, Xflat, lambda1=0.001, lambda2=0.001,\n",
    "            h_tol=1e-4, rho_max=1e+8\n",
    "        ) ## lambda1=0.01, lambda2=0.01, h_tol=1e-8, rho_max=1e+16\n",
    "        # assert ut.is_dag(W_notears)\n",
    "        # np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_notears != 0)\n",
    "        print('nCon: ', acc)\n",
    "        print(W_notears)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, nCon ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            should_std, trial_no, \n",
    "            acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']\n",
    "        )\n",
    "        file1.writelines(s1)\n",
    "        file1.close()    \n",
    "        #\n",
    "    except Exception as e:\n",
    "        print('========================================', e)\n",
    "        acc = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    ## initializing model and running the optimizaportion_parenttion\n",
    "    def conv_flat_to_con(A, concepts):\n",
    "\n",
    "        ##\n",
    "        A = np.abs(A) ## in the optimization this works on square matrix, so there we don't need to abs it\n",
    "        dflat = sum(concepts)\n",
    "        dcon = len(concepts)\n",
    "        Arow = np.zeros((dcon,dflat))\n",
    "        Ad = np.zeros((dcon,dcon))\n",
    "        end_concept = np.cumsum(concepts)\n",
    "\n",
    "        ##\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Arow[i,:] = (A[start_i:end_i,:].sum(axis=0))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Ad[:,i] = (Arow[:,start_i:end_i].sum(axis=1))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "\n",
    "        ##\n",
    "        new_adj_mat = np.zeros((dcon,dcon))\n",
    "        for i in range(dcon):\n",
    "            for j in range(dcon):\n",
    "                if Ad[i][j] != 0:\n",
    "                    new_adj_mat[i][j] = 1\n",
    "\n",
    "        return new_adj_mat\n",
    "\n",
    "    try:\n",
    "        model3 = nonlinear_old.NotearsMLP(dims=[dflat, 10, 1], bias=True)\n",
    "        W_notears3 = nonlinear_old.notears_nonlinear(\n",
    "            model3, Xflat, lambda1=0.001, lambda2=0.001, w_threshold=0.2,\n",
    "            h_tol=1e-4, rho_max=1e+8\n",
    "        ) ## lambda1=0.01, lambda2=0.01, w_threshold=0.3, h_tol=1e-8, rho_max=1e+16\n",
    "        W_notears3 = conv_flat_to_con(W_notears3, concepts)\n",
    "        # assert ut.is_dag(W_notears3)\n",
    "        # np.savetxt('outputs/W_notears3.csv', W_notears3, delimiter=',')\n",
    "        acc3 = ut.count_accuracy(B_true, W_notears3 != 0)\n",
    "        print('nRegFlat', acc3)\n",
    "        print(W_notears3)        \n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, nRegFlat ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            should_std, trial_no, \n",
    "            acc3['fdr'], acc3['tpr'], acc3['fpr'], acc3['shd'], acc3['nnz']\n",
    "        )                            \n",
    "        file1.writelines(s1)\n",
    "        file1.close()\n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc3 = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    #################################################\n",
    " \n",
    "    \n",
    "    return [\n",
    "        (acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']), \n",
    "        (acc3['fdr'], acc3['tpr'], acc3['fpr'], acc3['shd'], acc3['nnz']),        \n",
    "    ]\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    ## variables\n",
    "    list_should_std = [False, True]\n",
    "    n_trials = 2\n",
    "    \n",
    "    ## variables            \n",
    "    ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=56) ## detects automatically: num_cpus=64\n",
    "\n",
    "    ## experiments\n",
    "    for should_std in list_should_std:\n",
    "        list_result_id = []\n",
    "        for trial_no in range(n_trials):\n",
    "            result_id = get_result.remote(\n",
    "                df_x, df_cg, should_std, trial_no\n",
    "            )\n",
    "            list_result_id.append(result_id)\n",
    "        list_result = ray.get(list_result_id)\n",
    "\n",
    "        d_result = {}\n",
    "        for trial_no in range(n_trials):\n",
    "            d_result[(should_std, trial_no, 'nCon')] = list_result[trial_no][0]\n",
    "            d_result[(should_std, trial_no, 'nRegFlat')] = list_result[trial_no][1]                                \n",
    "\n",
    "        with open(\n",
    "            'datasets/d_result_' + str(should_std) + '.pickle', 'wb'\n",
    "        ) as handle: \n",
    "            pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
