{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept, nonlinear_old\n",
    "import igraph as ig\n",
    "# import lingam\n",
    "# from lingam.utils import make_prior_knowledge, make_dot\n",
    "import ray\n",
    "import pickle as pk\n",
    "\n",
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "## functions and classes\n",
    "\n",
    "def make_prior_knowledge_graph(prior_knowledge_matrix):\n",
    "    d = graphviz.Digraph(engine='dot')\n",
    "\n",
    "    labels = [f'x{i}' for i in range(prior_knowledge_matrix.shape[0])]\n",
    "    for label in labels:\n",
    "        d.node(label, label)\n",
    "\n",
    "    dirs = np.where(prior_knowledge_matrix > 0)\n",
    "    for to, from_ in zip(dirs[0], dirs[1]):\n",
    "        d.edge(labels[from_], labels[to])\n",
    "\n",
    "    dirs = np.where(prior_knowledge_matrix < 0)\n",
    "    for to, from_ in zip(dirs[0], dirs[1]):\n",
    "        if to != from_:\n",
    "            d.edge(labels[from_], labels[to], style='dashed')\n",
    "    return d\n",
    "\n",
    "def get_transformed_data(dim_input, dim_output, data_input, dt, hidden_unit):\n",
    "\n",
    "    class CustomNN(nn.Module):\n",
    "        def __init__(self, dt, hidden_unit):\n",
    "            super(CustomNN, self).__init__()\n",
    "            if dt=='linear':\n",
    "                self.nn_reg = nn.Sequential(\n",
    "                    nn.Linear(dim_input, dim_output),\n",
    "                )\n",
    "            else:\n",
    "                self.nn_reg = nn.Sequential(\n",
    "                    nn.Linear(dim_input, hidden_unit),\n",
    "                    nn.Sigmoid(),\n",
    "\n",
    "                    nn.Linear(hidden_unit, dim_output),\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.nn_reg(x)\n",
    "            return output\n",
    "        \n",
    "    model = CustomNN(dt, hidden_unit)\n",
    "    data_output = model(data_input)\n",
    "    return data_output\n",
    "\n",
    "def get_generated_data(con, B_true, dcon, n, param_scale, dt, hidden_unit):\n",
    "    dflat = sum(con)\n",
    "    G = ig.Graph.Adjacency(B_true.tolist())\n",
    "    ordered_vertices = G.topological_sorting()  \n",
    "    assert len(ordered_vertices) == dcon\n",
    "\n",
    "    dict_new_x = {}\n",
    "    for v_index in ordered_vertices:\n",
    "        col = B_true[:, v_index]\n",
    "        col_sum = np.sum(col, axis=0)\n",
    "        if col_sum == 0:\n",
    "            portion_parent = 0\n",
    "        else:\n",
    "            dim_output = con[v_index] ## con holds the dimension of output\n",
    "            dim_input = 0\n",
    "            data_input = None\n",
    "            i=0\n",
    "            for row in col:\n",
    "                if row == 1:\n",
    "                    dim_input += con[i]\n",
    "                    if data_input is None:\n",
    "                        data_input = dict_new_x[i]\n",
    "                    else:\n",
    "                        data_input = torch.cat([data_input, dict_new_x[i]], dim=1) \n",
    "                i+=1\n",
    "\n",
    "            data_output = get_transformed_data(dim_input, dim_output, data_input, dt, hidden_unit)\n",
    "            portion_parent = data_output.detach()\n",
    "\n",
    "        portion_noise = torch.randn(n, con[v_index])\n",
    "        if col_sum == 0:\n",
    "            new_x = param_scale * portion_noise\n",
    "        else:\n",
    "            new_x = param_scale * portion_parent + portion_noise\n",
    "        dict_new_x[v_index] = new_x\n",
    "\n",
    "    Xf = dict_new_x[0]\n",
    "    for i in range(1, dcon):\n",
    "        Xf = np.hstack((Xf, dict_new_x[i]))\n",
    "    return Xf\n",
    "\n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(\n",
    "    dt, st, n, d, s0_factor, gt, should_std, trial_no\n",
    "):\n",
    "    ## (1a) variable setup\n",
    "    np.random.seed(123+trial_no) \n",
    "    ut.set_random_seed(123+trial_no)                            \n",
    "    s0 = d * s0_factor\n",
    "    dcon = d                            \n",
    "    concept_dim_limit=3\n",
    "    param_scale = d\n",
    "    hidden_unit = 100  \n",
    "    #################################################\n",
    "\n",
    "    ## (1b) generate a causal graph at random as you have done already (eg. x1->x2) \n",
    "    ##     but this time it will represent relations between concepts,\n",
    "    B_true = ut.simulate_dag(d, s0, gt)                            \n",
    "    folder_name = str(dt) + '_n_d_s0_gt_sem_' \\\n",
    "                    + str(n) + '_' + str(d) + '_' \\\n",
    "                        + str(s0) + '_' + str(gt) + '_' + str(st)\n",
    "    folder_path = 'datasets/' + folder_name + '/'\n",
    "    if os.path.exists(folder_path):\n",
    "        pass \n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "    file_name = str(trial_no) + '_W_true.csv'\n",
    "    file_path = folder_path + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        B_true = genfromtxt(file_path, delimiter=',')\n",
    "    else:                                \n",
    "        np.savetxt(file_path, B_true, delimiter=',')                            \n",
    "    #################################################\n",
    "\n",
    "    ## (2) randomly decide the embedding size of your concepts (eg. dim(x1)=3, dim(x2)=5).\n",
    "    ##     generate the extended true graph in 'dflat' level.\n",
    "    concepts = torch.randint(1, concept_dim_limit+1, (dcon,)) \n",
    "    concepts = [int(i) for i in concepts]\n",
    "    print('printing concepts: ', concepts)\n",
    "    # concepts_fake = torch.randint(1, 2, (dcon,)) \n",
    "    # concepts_fake = [int(i) for i in concepts_fake]\n",
    "    # print('printing concepts_fake: ',concepts_fake )\n",
    "    dflat = sum(concepts)\n",
    "    B_true_2 = np.zeros((dflat, dflat))\n",
    "    end_concept = np.cumsum(concepts)\n",
    "    for i in range(dcon):\n",
    "        for j in range(dcon):\n",
    "            if B_true[i][j]==1:\n",
    "\n",
    "                if i==0:\n",
    "                    from_start=0\n",
    "                else:\n",
    "                    from_start= end_concept[i-1]\n",
    "                from_end = end_concept[i]\n",
    "\n",
    "                if j==0:\n",
    "                    to_start=0\n",
    "                else:\n",
    "                    to_start=end_concept[j-1]\n",
    "                to_end=end_concept[j]\n",
    "\n",
    "                for from_x in range(from_start, from_end):\n",
    "                    for to_x in range(to_start, to_end):\n",
    "                        B_true_2[from_x][to_x]=1 \n",
    "    file_name = str(trial_no) + '_W_true_2.csv'\n",
    "    file_path = folder_path + file_name\n",
    "    # if os.path.exists(file_path):\n",
    "        # B_true_2 = genfromtxt(file_path, delimiter=',')\n",
    "    # else:                                \n",
    "        # np.savetxt(file_path, B_true_2, delimiter=',')\n",
    "    np.savetxt(file_path, B_true_2, delimiter=',')\n",
    "    #################################################\n",
    "\n",
    "    ## (3) generate a list of neural networks for each effect concept (eg. nn_x2 (input=3, output=5, weights=random), \n",
    "    ## (4) generate data for x1 = randn(dim=3) for x2 = nn_x2(x1) + eps*rand(dim=5)\n",
    "\n",
    "    X = get_generated_data(concepts, B_true, dcon, n, param_scale, dt, hidden_unit)\n",
    "    file_name = str(trial_no) + '_X.csv'\n",
    "    file_path = folder_path + file_name\n",
    "    # if os.path.exists(file_path):\n",
    "    #     X = genfromtxt(file_path, delimiter=',')\n",
    "    # else:                                \n",
    "    #     np.savetxt(file_path, X, delimiter=',')\n",
    "    np.savetxt(file_path, X, delimiter=',')\n",
    "    ##\n",
    "    # X_fake = get_generated_data(concepts_fake, B_true, dcon, n, param_scale, dt, hidden_unit)                                \n",
    "    # file_name = str(trial_no) + '_X_fake.csv'\n",
    "    # file_path = folder_path + file_name\n",
    "    # np.savetxt(file_path, X_fake, delimiter=',')\n",
    "    if should_std:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)    \n",
    "    #################################################\n",
    "\n",
    "    ## (5) run exp\n",
    "    mask = np.ones((dcon, dcon)) * np.nan\n",
    "    # n, dflat = X.shape[0], X.shape[1]\n",
    "    print(concepts, dcon, dflat)\n",
    "    assert len(concepts) == dcon \n",
    "    assert sum(concepts) == dflat\n",
    "    assert X.shape[1] == dflat    \n",
    "\n",
    "    ## initializing model and running the optimization\n",
    "    try:\n",
    "        metainfo = {}\n",
    "        metainfo['dflat'] = dflat\n",
    "        metainfo['dcon'] = dcon\n",
    "        metainfo['concepts'] = concepts                            \n",
    "        model = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dflat, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=0.2, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo\n",
    "        )\n",
    "        W_notears, res = nonlinear_concept.notears_nonlinear(model, X, lambda1=0.001, lambda2=0.001) ## lambda1=0.01, lambda2=0.01\n",
    "        # assert ut.is_dag(W_notears)\n",
    "        # np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true_2, W_notears != 0)\n",
    "        print('nCon: ', acc)\n",
    "        print(W_notears)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, {}, {}, {}, {}, nCon ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            n, d, s0_factor, gt, should_std, trial_no, \n",
    "            acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']\n",
    "        )\n",
    "        file1.writelines(s1)\n",
    "        file1.close()    \n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "    ## initializing model and running the optimization\n",
    "    try:\n",
    "        model2 = nonlinear_old.NotearsMLP(dims=[dflat, 10, 1], bias=True)\n",
    "        W_notears2 = nonlinear_old.notears_nonlinear(model2, X, lambda1=0.001, lambda2=0.001, w_threshold=0.2) ## lambda1=0.01, lambda2=0.01, w_threshold=0.3\n",
    "        # assert ut.is_dag(W_notears2)\n",
    "        # np.savetxt('outputs/W_notears2.csv', W_notears2, delimiter=',')\n",
    "        acc2 = ut.count_accuracy(B_true_2, W_notears2 != 0)\n",
    "        print('n_reg', acc2)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, {}, {}, {}, {}, nReg ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            n, d, s0_factor, gt, should_std, trial_no, \n",
    "            acc2['fdr'], acc2['tpr'], acc2['fpr'], acc2['shd'], acc2['nnz']\n",
    "        )                            \n",
    "        file1.writelines(s1)\n",
    "        file1.close()\n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc2 = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "    \n",
    "    #################################################\n",
    "    \n",
    "    return [\n",
    "        (acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']), \n",
    "        (acc2['fdr'], acc2['tpr'], acc2['fpr'], acc2['shd'], acc2['nnz']), \n",
    "    ]\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    ## variables\n",
    "\n",
    "    #\n",
    "    list_dt_st = [('nonlinear', 'mlp')] ## [('nonlinear', 'mlp'), ('linear', 'mlp')]\n",
    "    list_n = [200, 1000] ## [200, 1000]\n",
    "    list_d = [10, 20] ## [10, 20]\n",
    "    list_s0_factor = [1, 4] ## [1, 4]\n",
    "    list_gt = ['ER', 'SF'] ## ['ER', 'SF']\n",
    "    list_should_std = [False, True] ## [False, True]\n",
    "    n_trials = 50 ## 10\n",
    "    #\n",
    "    \n",
    "    ## experiments\n",
    "\n",
    "    ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=16) ## detects automatically: num_cpus=64\n",
    "\n",
    "    for dt, st in list_dt_st:\n",
    "        for n in list_n:\n",
    "            for d in list_d:\n",
    "                for s0_factor in list_s0_factor:\n",
    "                    for gt in list_gt:\n",
    "                        for should_std in list_should_std:\n",
    "\n",
    "                            list_result_id = []\n",
    "                            for trial_no in range(n_trials):\n",
    "                                result_id = get_result.remote(\n",
    "                                    dt, st, n, d, s0_factor, gt, should_std, trial_no\n",
    "                                )\n",
    "                                list_result_id.append(result_id)\n",
    "                            list_result = ray.get(list_result_id)\n",
    "\n",
    "                            d_result = {}\n",
    "                            for trial_no in range(n_trials):\n",
    "                                d_result[(n, d, s0_factor, gt, should_std, trial_no, 'nCon')] = list_result[trial_no][0]\n",
    "                                d_result[(n, d, s0_factor, gt, should_std, trial_no, 'nReg')] = list_result[trial_no][1]\n",
    "\n",
    "                            with open(\n",
    "                                'datasets/d_result_' + str(n) + '_' + str(d) + '_' + str(s0_factor) + '_' + str(gt) + '_' + str(should_std) + '.pickle', 'wb'\n",
    "                            ) as handle: \n",
    "                                pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
