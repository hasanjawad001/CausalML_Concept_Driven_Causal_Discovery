{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept\n",
    "import igraph as ig\n",
    "import lingam\n",
    "from lingam.utils import make_prior_knowledge, make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.23.3', '1.4.2', '0.20.1', '1.7.0']\n"
     ]
    }
   ],
   "source": [
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__, graphviz.__version__, lingam.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions and classes\n",
    "\n",
    "def make_prior_knowledge_graph(prior_knowledge_matrix):\n",
    "    d = graphviz.Digraph(engine='dot')\n",
    "\n",
    "    labels = [f'x{i}' for i in range(prior_knowledge_matrix.shape[0])]\n",
    "    for label in labels:\n",
    "        d.node(label, label)\n",
    "\n",
    "    dirs = np.where(prior_knowledge_matrix > 0)\n",
    "    for to, from_ in zip(dirs[0], dirs[1]):\n",
    "        d.edge(labels[from_], labels[to])\n",
    "\n",
    "    dirs = np.where(prior_knowledge_matrix < 0)\n",
    "    for to, from_ in zip(dirs[0], dirs[1]):\n",
    "        if to != from_:\n",
    "            d.edge(labels[from_], labels[to], style='dashed')\n",
    "    return d\n",
    "\n",
    "def get_transformed_data(dim_input, dim_output, data_input, dt):\n",
    "\n",
    "    class CustomNN(nn.Module):\n",
    "        def __init__(self, dt):\n",
    "            super(CustomNN, self).__init__()\n",
    "            if dt=='linear':\n",
    "                self.nn_reg = nn.Sequential(\n",
    "                    nn.Linear(dim_input, dim_output),\n",
    "                )\n",
    "            else:\n",
    "                self.nn_reg = nn.Sequential(\n",
    "                    nn.Linear(dim_input, hidden_unit),\n",
    "                    nn.Sigmoid(),\n",
    "\n",
    "                    nn.Linear(hidden_unit, dim_output),\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.nn_reg(x)\n",
    "            return output\n",
    "        \n",
    "    model = CustomNN(dt)\n",
    "    data_output = model(data_input)\n",
    "    return data_output\n",
    "\n",
    "def get_generated_data(con, B_true, dcon, n, param_scale, dt):\n",
    "    dflat = sum(con)\n",
    "    G = ig.Graph.Adjacency(B_true.tolist())\n",
    "    ordered_vertices = G.topological_sorting()  \n",
    "    assert len(ordered_vertices) == dcon\n",
    "\n",
    "    dict_new_x = {}\n",
    "    for v_index in ordered_vertices:\n",
    "        col = B_true[:, v_index]\n",
    "        col_sum = np.sum(col, axis=0)\n",
    "        if col_sum == 0:\n",
    "            portion_parent = 0\n",
    "        else:\n",
    "            dim_output = con[v_index] ## con holds the dimension of output\n",
    "            dim_input = 0\n",
    "            data_input = None\n",
    "            i=0\n",
    "            for row in col:\n",
    "                if row == 1:\n",
    "                    dim_input += con[i]\n",
    "                    if data_input is None:\n",
    "                        data_input = dict_new_x[i]\n",
    "                    else:\n",
    "                        data_input = torch.cat([data_input, dict_new_x[i]], dim=1) \n",
    "                i+=1\n",
    "\n",
    "            data_output = get_transformed_data(dim_input, dim_output, data_input, dt)\n",
    "            portion_parent = data_output.detach()\n",
    "\n",
    "        portion_noise = torch.randn(n, con[v_index])\n",
    "        if col_sum == 0:\n",
    "            new_x = portion_noise\n",
    "        else:\n",
    "            new_x = portion_parent + portion_noise\n",
    "        dict_new_x[v_index] = param_scale * new_x\n",
    "\n",
    "    Xf = dict_new_x[0]\n",
    "    for i in range(1, dcon):\n",
    "        Xf = np.hstack((Xf, dict_new_x[i]))\n",
    "    return Xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables\n",
    "\n",
    "list_dt_st = [('nonlinear', 'mlp')] ## [('nonlinear', 'mlp'), ('linear', 'mlp')]\n",
    "list_n = [200, 1000] ## [200, 1000]\n",
    "list_d = [5, 10] ## [10, 20]\n",
    "list_s0_factor = [1, 2] ## [1, 4]\n",
    "list_gt = ['ER', 'SF'] ## ['ER', 'SF']\n",
    "list_should_std = [False, True] ## [False, True]\n",
    "n_trials = 10 ## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## experiments\n",
    "\n",
    "d_result = {}\n",
    "for dt, st in list_dt_st:\n",
    "    for n in list_n:\n",
    "        for d in list_d:\n",
    "            for s0_factor in list_s0_factor:\n",
    "                for gt in list_gt:\n",
    "                    for should_std in list_should_std:\n",
    "                        for trial_no in range(n_trials):\n",
    "                            \n",
    "                            ## (1a) variable setup\n",
    "                            np.random.seed(123+trial_no) \n",
    "                            ut.set_random_seed(123+trial_no)                            \n",
    "                            s0 = d * s0_factor\n",
    "                            dcon = d                            \n",
    "                            concept_dim_limit=d\n",
    "                            param_scale = d\n",
    "                            hidden_unit = 100  \n",
    "                            #################################################\n",
    "                            \n",
    "                            ## (1b) generate a causal graph at random as you have done already (eg. x1->x2) \n",
    "                            ##     but this time it will represent relations between concepts,\n",
    "                            B_true = ut.simulate_dag(d, s0, gt)                            \n",
    "                            folder_name = str(dt) + '_n_d_s0_gt_sem_' \\\n",
    "                                            + str(n) + '_' + str(d) + '_' \\\n",
    "                                                + str(s0) + '_' + str(gt) + '_' + str(st)\n",
    "                            folder_path = 'datasets/' + folder_name + '/'\n",
    "                            if os.path.exists(folder_path):\n",
    "                                pass \n",
    "                            else:\n",
    "                                os.makedirs(folder_path)\n",
    "                            file_name = str(trial_no) + '_W_true.csv'\n",
    "                            file_path = folder_path + file_name\n",
    "                            if os.path.exists(file_path):\n",
    "                                B_true = genfromtxt(file_path, delimiter=',')\n",
    "                            else:                                \n",
    "                                np.savetxt(file_path, B_true, delimiter=',')                            \n",
    "                            #################################################\n",
    "                            \n",
    "                            ## (2) randomly decide the embedding size of your concepts (eg. dim(x1)=3, dim(x2)=5). \n",
    "                            concepts = torch.randint(1, concept_dim_limit+1, (dcon,)) \n",
    "                            concepts = [int(i) for i in concepts]\n",
    "                            print('printing concepts: ', concepts)\n",
    "                            concepts_fake = torch.randint(1, 2, (dcon,)) \n",
    "                            concepts_fake = [int(i) for i in concepts_fake]\n",
    "                            print('printing concepts_fake: ',concepts_fake )      \n",
    "                            #################################################\n",
    "                                                        \n",
    "                            ## (3) generate a list of neural networks for each effect concept (eg. nn_x2 (input=3, output=5, weights=random), \n",
    "                            ## (4) generate data for x1 = randn(dim=3) for x2 = nn_x2(x1) + eps*rand(dim=5)\n",
    "                            \n",
    "                            X = get_generated_data(concepts, B_true, dcon, n, param_scale, dt)\n",
    "                            file_name = str(trial_no) + '_X.csv'\n",
    "                            file_path = folder_path + file_name\n",
    "                            # if os.path.exists(file_path):\n",
    "                            #     X = genfromtxt(file_path, delimiter=',')\n",
    "                            # else:                                \n",
    "                            #     np.savetxt(file_path, X, delimiter=',')\n",
    "                            np.savetxt(file_path, X, delimiter=',')\n",
    "                            ##\n",
    "                            X_fake = get_generated_data(concepts_fake, B_true, dcon, n, param_scale, dt)                                \n",
    "                            file_name = str(trial_no) + '_X_fake.csv'\n",
    "                            file_path = folder_path + file_name\n",
    "                            np.savetxt(file_path, X_fake, delimiter=',')\n",
    "                            #################################################\n",
    "                            \n",
    "                            ## (5) run exp\n",
    "                            mask = np.ones((dcon, dcon)) * np.nan\n",
    "                            n, dflat = X.shape[0], X.shape[1]\n",
    "                            print(concepts, dcon, dflat)\n",
    "                            assert len(concepts) == dcon \n",
    "                            assert sum(concepts) == dflat\n",
    "                            \n",
    "                            ## initializing model and running the optimization\n",
    "                            metainfo = {}\n",
    "                            metainfo['dflat'] = dflat\n",
    "                            metainfo['dcon'] = dcon\n",
    "                            metainfo['concepts'] = concepts                            \n",
    "                            model = nonlinear_concept.NotearsMLP(\n",
    "                                dims=[dflat, 10, 1], bias=True,\n",
    "                                mask=mask, w_threshold=0.3, learned_model=None,\n",
    "                                metainfo=metainfo\n",
    "                            )\n",
    "                            if should_std:\n",
    "                                scaler = StandardScaler().fit(X)\n",
    "                                X = scaler.transform(X)\n",
    "                            W_notears, res = nonlinear_concept.notears_nonlinear(model, X, lambda1=0.01, lambda2=0.01)\n",
    "                            # assert ut.is_dag(W_notears)\n",
    "                            # np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "                            acc = ut.count_accuracy(B_true, W_notears != 0)\n",
    "                            print('X: ', acc)\n",
    "                            #\n",
    "                            file1 = open('logger.log', 'a+')  \n",
    "                            s1 = \"{}, {}, {}, {}, {}, {}, X ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "                                n, d, s0_factor, gt, should_std, trial_nol_no, \n",
    "                                acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']\n",
    "                            )\n",
    "                            file1.writelines(s1)\n",
    "                            file1.close()                    \n",
    "                            #\n",
    "\n",
    "                            \n",
    "                            ## initializing model and running the optimization\n",
    "                            metainfo2 = {}\n",
    "                            metainfo2['dflat'] = dcon\n",
    "                            metainfo2['dcon'] = dcon\n",
    "                            metainfo2['concepts'] = concepts_fake\n",
    "                            model2 = nonlinear_concept.NotearsMLP(\n",
    "                                dims=[dcon, 10, 1], bias=True,\n",
    "                                mask=mask, w_threshold=0.3, learned_model=None,\n",
    "                                metainfo=metainfo2\n",
    "                            )\n",
    "                            if should_std:\n",
    "                                scaler = StandardScaler().fit(X_fake)\n",
    "                                X_fake = scaler.transform(X_fake)                            \n",
    "                            W_notears2, res2 = nonlinear_concept.notears_nonlinear(model2, X_fake, lambda1=0.01, lambda2=0.01)\n",
    "                            # assert ut.is_dag(W_notears2)\n",
    "                            # np.savetxt('outputs/W_notears2.csv', W_notears2, delimiter=',')\n",
    "                            acc2 = ut.count_accuracy(B_true, W_notears2 != 0)\n",
    "                            print('X_fake', acc2)\n",
    "                            #\n",
    "                            file1 = open('logger.log', 'a+')  \n",
    "                            s1 = \"{}, {}, {}, {}, {}, {}, X_fake ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "                                n, d, s0_factor, gt, should_std, trial_no, \n",
    "                                acc2['fdr'], acc2['tpr'], acc2['fpr'], acc2['shd'], acc2['nnz']\n",
    "                            )                            \n",
    "                            file1.writelines(s1)\n",
    "                            file1.close()\n",
    "                            #\n",
    "                            #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
