{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.21.3', '1.0.1']\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m printing concepts:  [1, 2, 1, 1, 3]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m printing concepts_fake:  [1, 1, 1, 1, 1]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m [1, 2, 1, 1, 3] 5 8\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  0\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  1\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  2\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  3\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  4\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  5\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m X:  {'fdr': 0.875, 'tpr': 0.2, 'fpr': 1.4, 'shd': 9, 'nnz': 8}\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m [[0.         9.2360115  0.59223694 0.         2.187346  ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         0.         0.         0.         0.        ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         5.497279   0.         0.         0.2213115 ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         7.6574264  0.         0.         0.5898009 ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         7.4761853  0.         0.         0.        ]]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  0\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  1\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  2\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  3\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  4\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m X_fake {'fdr': 0.7142857142857143, 'tpr': 0.4, 'fpr': 1.0, 'shd': 6, 'nnz': 7}\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m printing concepts:  [1, 2, 3, 2, 3]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m printing concepts_fake:  [1, 1, 1, 1, 1]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m [1, 2, 3, 2, 3] 5 11\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  0\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  1\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  2\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  3\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  4\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  5\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  6\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m X:  {'fdr': 0.875, 'tpr': 0.1, 'fpr': 7.0, 'shd': 9, 'nnz': 8}\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m [[0.         6.001874   0.         0.         0.        ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         0.         0.         0.         0.        ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.28860927 6.003386   0.         0.6723973  0.        ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         6.563264   0.         0.         0.        ]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m  [0.         7.273674   0.529626   1.3154036  0.        ]]\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  0\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  1\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  2\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  3\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  4\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m -----iteration no:  5\n",
      "\u001b[2m\u001b[36m(pid=39870)\u001b[0m X_fake {'fdr': 0.7142857142857143, 'tpr': 0.2, 'fpr': 5.0, 'shd': 8, 'nnz': 7}\n"
     ]
    }
   ],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept\n",
    "import igraph as ig\n",
    "# import lingam\n",
    "# from lingam.utils import make_prior_knowledge, make_dot\n",
    "import ray\n",
    "import pickle as pk\n",
    "\n",
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "## functions and classes\n",
    "\n",
    "def make_prior_knowledge_graph(prior_knowledge_matrix):\n",
    "    d = graphviz.Digraph(engine='dot')\n",
    "\n",
    "    labels = [f'x{i}' for i in range(prior_knowledge_matrix.shape[0])]\n",
    "    for label in labels:\n",
    "        d.node(label, label)\n",
    "\n",
    "    dirs = np.where(prior_knowledge_matrix > 0)\n",
    "    for to, from_ in zip(dirs[0], dirs[1]):\n",
    "        d.edge(labels[from_], labels[to])\n",
    "\n",
    "    dirs = np.where(prior_knowledge_matrix < 0)\n",
    "    for to, from_ in zip(dirs[0], dirs[1]):\n",
    "        if to != from_:\n",
    "            d.edge(labels[from_], labels[to], style='dashed')\n",
    "    return d\n",
    "\n",
    "def get_transformed_data(dim_input, dim_output, data_input, dt, hidden_unit):\n",
    "\n",
    "    class CustomNN(nn.Module):\n",
    "        def __init__(self, dt, hidden_unit):\n",
    "            super(CustomNN, self).__init__()\n",
    "            if dt=='linear':\n",
    "                self.nn_reg = nn.Sequential(\n",
    "                    nn.Linear(dim_input, dim_output),\n",
    "                )\n",
    "            else:\n",
    "                self.nn_reg = nn.Sequential(\n",
    "                    nn.Linear(dim_input, hidden_unit),\n",
    "                    nn.Sigmoid(),\n",
    "\n",
    "                    nn.Linear(hidden_unit, dim_output),\n",
    "                )\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.nn_reg(x)\n",
    "            return output\n",
    "        \n",
    "    model = CustomNN(dt, hidden_unit)\n",
    "    data_output = model(data_input)\n",
    "    return data_output\n",
    "\n",
    "def get_generated_data(con, B_true, dcon, n, param_scale, dt, hidden_unit):\n",
    "    dflat = sum(con)\n",
    "    G = ig.Graph.Adjacency(B_true.tolist())\n",
    "    ordered_vertices = G.topological_sorting()  \n",
    "    assert len(ordered_vertices) == dcon\n",
    "\n",
    "    dict_new_x = {}\n",
    "    for v_index in ordered_vertices:\n",
    "        col = B_true[:, v_index]\n",
    "        col_sum = np.sum(col, axis=0)\n",
    "        if col_sum == 0:\n",
    "            portion_parent = 0\n",
    "        else:\n",
    "            dim_output = con[v_index] ## con holds the dimension of output\n",
    "            dim_input = 0\n",
    "            data_input = None\n",
    "            i=0\n",
    "            for row in col:\n",
    "                if row == 1:\n",
    "                    dim_input += con[i]\n",
    "                    if data_input is None:\n",
    "                        data_input = dict_new_x[i]\n",
    "                    else:\n",
    "                        data_input = torch.cat([data_input, dict_new_x[i]], dim=1) \n",
    "                i+=1\n",
    "\n",
    "            data_output = get_transformed_data(dim_input, dim_output, data_input, dt, hidden_unit)\n",
    "            portion_parent = data_output.detach()\n",
    "\n",
    "        portion_noise = torch.randn(n, con[v_index])\n",
    "        if col_sum == 0:\n",
    "            new_x = param_scale * portion_noise\n",
    "        else:\n",
    "            new_x = param_scale * portion_parent + portion_noise\n",
    "        dict_new_x[v_index] = new_x\n",
    "\n",
    "    Xf = dict_new_x[0]\n",
    "    for i in range(1, dcon):\n",
    "        Xf = np.hstack((Xf, dict_new_x[i]))\n",
    "    return Xf\n",
    "\n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(\n",
    "    dt, st, n, d, s0_factor, gt, should_std, trial_no\n",
    "):\n",
    "    ## (1a) variable setup\n",
    "    np.random.seed(123+trial_no) \n",
    "    ut.set_random_seed(123+trial_no)                            \n",
    "    s0 = d * s0_factor\n",
    "    dcon = d                            \n",
    "    concept_dim_limit=3\n",
    "    param_scale = d\n",
    "    hidden_unit = 100  \n",
    "    #################################################\n",
    "\n",
    "    ## (1b) generate a causal graph at random as you have done already (eg. x1->x2) \n",
    "    ##     but this time it will represent relations between concepts,\n",
    "    B_true = ut.simulate_dag(d, s0, gt)                            \n",
    "    folder_name = str(dt) + '_n_d_s0_gt_sem_' \\\n",
    "                    + str(n) + '_' + str(d) + '_' \\\n",
    "                        + str(s0) + '_' + str(gt) + '_' + str(st)\n",
    "    folder_path = 'datasets/' + folder_name + '/'\n",
    "    if os.path.exists(folder_path):\n",
    "        pass \n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "    file_name = str(trial_no) + '_W_true.csv'\n",
    "    file_path = folder_path + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        B_true = genfromtxt(file_path, delimiter=',')\n",
    "    else:                                \n",
    "        np.savetxt(file_path, B_true, delimiter=',')                            \n",
    "    #################################################\n",
    "\n",
    "    ## (2) randomly decide the embedding size of your concepts (eg. dim(x1)=3, dim(x2)=5). \n",
    "    concepts = torch.randint(1, concept_dim_limit+1, (dcon,)) \n",
    "    concepts = [int(i) for i in concepts]\n",
    "    print('printing concepts: ', concepts)\n",
    "    concepts_fake = torch.randint(1, 2, (dcon,)) \n",
    "    concepts_fake = [int(i) for i in concepts_fake]\n",
    "    print('printing concepts_fake: ',concepts_fake )      \n",
    "    #################################################\n",
    "\n",
    "    ## (3) generate a list of neural networks for each effect concept (eg. nn_x2 (input=3, output=5, weights=random), \n",
    "    ## (4) generate data for x1 = randn(dim=3) for x2 = nn_x2(x1) + eps*rand(dim=5)\n",
    "\n",
    "    X = get_generated_data(concepts, B_true, dcon, n, param_scale, dt, hidden_unit)\n",
    "    file_name = str(trial_no) + '_X.csv'\n",
    "    file_path = folder_path + file_name\n",
    "    # if os.path.exists(file_path):\n",
    "    #     X = genfromtxt(file_path, delimiter=',')\n",
    "    # else:                                \n",
    "    #     np.savetxt(file_path, X, delimiter=',')\n",
    "    np.savetxt(file_path, X, delimiter=',')\n",
    "    ##\n",
    "    X_fake = get_generated_data(concepts_fake, B_true, dcon, n, param_scale, dt, hidden_unit)                                \n",
    "    file_name = str(trial_no) + '_X_fake.csv'\n",
    "    file_path = folder_path + file_name\n",
    "    np.savetxt(file_path, X_fake, delimiter=',')\n",
    "    #################################################\n",
    "\n",
    "    ## (5) run exp\n",
    "    mask = np.ones((dcon, dcon)) * np.nan\n",
    "    n, dflat = X.shape[0], X.shape[1]\n",
    "    print(concepts, dcon, dflat)\n",
    "    assert len(concepts) == dcon \n",
    "    assert sum(concepts) == dflat\n",
    "\n",
    "    ## initializing model and running the optimization\n",
    "    try:\n",
    "        metainfo = {}\n",
    "        metainfo['dflat'] = dflat\n",
    "        metainfo['dcon'] = dcon\n",
    "        metainfo['concepts'] = concepts                            \n",
    "        model = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dflat, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=0.2, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo\n",
    "        )\n",
    "        if should_std:\n",
    "            scaler = StandardScaler().fit(X)\n",
    "            X = scaler.transform(X)\n",
    "        W_notears, res = nonlinear_concept.notears_nonlinear(model, X, lambda1=0.001, lambda2=0.001) ## lambda1=0.01, lambda2=0.01\n",
    "        # assert ut.is_dag(W_notears)\n",
    "        # np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_notears != 0)\n",
    "        print('X: ', acc)\n",
    "        print(W_notears)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, {}, {}, {}, {}, X ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            n, d, s0_factor, gt, should_std, trial_no, \n",
    "            acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']\n",
    "        )\n",
    "        file1.writelines(s1)\n",
    "        file1.close()    \n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "    ## initializing model and running the optimization\n",
    "    try:\n",
    "        metainfo2 = {}\n",
    "        metainfo2['dflat'] = dcon\n",
    "        metainfo2['dcon'] = dcon\n",
    "        metainfo2['concepts'] = concepts_fake\n",
    "        model2 = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dcon, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=0.2, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo2\n",
    "        )\n",
    "        if should_std:\n",
    "            scaler = StandardScaler().fit(X_fake)\n",
    "            X_fake = scaler.transform(X_fake)                            \n",
    "        W_notears2, res2 = nonlinear_concept.notears_nonlinear(model2, X_fake, lambda1=0.001, lambda2=0.001) ## lambda1=0.01, lambda2=0.01\n",
    "        # assert ut.is_dag(W_notears2)\n",
    "        # np.savetxt('outputs/W_notears2.csv', W_notears2, delimiter=',')\n",
    "        acc2 = ut.count_accuracy(B_true, W_notears2 != 0)\n",
    "        print('X_fake', acc2)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, {}, {}, {}, {}, X_fake ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            n, d, s0_factor, gt, should_std, trial_no, \n",
    "            acc2['fdr'], acc2['tpr'], acc2['fpr'], acc2['shd'], acc2['nnz']\n",
    "        )                            \n",
    "        file1.writelines(s1)\n",
    "        file1.close()\n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc2 = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "    \n",
    "    #################################################\n",
    "    \n",
    "    return [\n",
    "        (acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']), \n",
    "        (acc2['fdr'], acc2['tpr'], acc2['fpr'], acc2['shd'], acc2['nnz']), \n",
    "    ]\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    ## variables\n",
    "\n",
    "    #\n",
    "    list_dt_st = [('nonlinear', 'mlp')] ## [('nonlinear', 'mlp'), ('linear', 'mlp')]\n",
    "    list_n = [200, 1000] ## [200, 1000]\n",
    "    list_d = [5, 10] ## [10, 20]\n",
    "    list_s0_factor = [1, 2] ## [1, 4]\n",
    "    list_gt = ['ER', 'SF'] ## ['ER', 'SF']\n",
    "    list_should_std = [False, True] ## [False, True]\n",
    "    n_trials = 10 ## 10\n",
    "    #\n",
    "    \n",
    "    ## experiments\n",
    "\n",
    "    ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=16) ## detects automatically: num_cpus=64\n",
    "\n",
    "    for dt, st in list_dt_st:\n",
    "        for n in list_n:\n",
    "            for d in list_d:\n",
    "                for s0_factor in list_s0_factor:\n",
    "                    for gt in list_gt:\n",
    "                        for should_std in list_should_std:\n",
    "\n",
    "                            list_result_id = []\n",
    "                            for trial_no in range(n_trials):\n",
    "                                result_id = get_result.remote(\n",
    "                                    dt, st, n, d, s0_factor, gt, should_std, trial_no\n",
    "                                )\n",
    "                                list_result_id.append(result_id)\n",
    "                            list_result = ray.get(list_result_id)\n",
    "\n",
    "                            d_result = {}\n",
    "                            for trial_no in range(n_trials):\n",
    "                                d_result[(n, d, s0_factor, gt, should_std, trial_no, 'X')] = list_result[trial_no][0]\n",
    "                                d_result[(n, d, s0_factor, gt, should_std, trial_no, 'X_fake')] = list_result[trial_no][1]\n",
    "\n",
    "                            with open(\n",
    "                                'datasets/d_result_' + str(n) + '_' + str(d) + '_' + str(s0_factor) + '_' + str(gt) + '_' + str(should_std) + '.pickle', 'wb'\n",
    "                            ) as handle: \n",
    "                                pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
