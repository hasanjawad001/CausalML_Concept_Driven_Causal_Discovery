{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.23.3', '1.4.2']\n"
     ]
    }
   ],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept, nonlinear_old\n",
    "import igraph as ig\n",
    "# import lingam\n",
    "# from lingam.utils import make_prior_knowledge, make_dot\n",
    "import ray\n",
    "import pickle as pk\n",
    "from scipy.special import expit as sigmoid\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import math\n",
    "\n",
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(df_x, opt):\n",
    "        \n",
    "    ## 1\n",
    "    should_std, val_lambda, w_threshold = opt[0], opt[1], opt[2]    \n",
    "    np.random.seed(0) \n",
    "    ut.set_random_seed(0) \n",
    "\n",
    "    ## 2\n",
    "\n",
    "    # 'budget', \n",
    "    # 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10', 'w11', 'w12', \n",
    "    # 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10', \n",
    "    # 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', \n",
    "    # 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'c10', 'c11', 'c12', 'c13', 'c14', 'c15', 'c16', 'c17', \n",
    "    # 'g1','g2', 'g3', 'g4', 'g5', \n",
    "    # 'metascore', \n",
    "    # 'imdb_user_rating', \n",
    "    # 'week_of_year',\n",
    "    # 'revenue'\n",
    "#     concepts = [1, 12, 10, 13, 17, 5, 1, 1, 1, 1] \n",
    "    concepts = [1, 17, 5, 1, 1] ## bud, cast, genre, imdb, rev   \n",
    "    Xflat = df_x.values\n",
    "    \n",
    "    ## 3\n",
    "    if should_std:\n",
    "        scalerFlat = StandardScaler().fit(Xflat)\n",
    "        Xflat = scalerFlat.transform(Xflat)    \n",
    "    Xflat = Xflat.astype('float32')\n",
    "    n, dflat = Xflat.shape\n",
    "    dcon = len(concepts)\n",
    "    \n",
    "    ## 4\n",
    "    mask = np.ones((dcon, dcon)) * np.nan\n",
    "    print(concepts, dcon, dflat)\n",
    "    assert len(concepts) == dcon \n",
    "    assert sum(concepts) == dflat\n",
    "    assert Xflat.shape[1] == dflat    \n",
    "\n",
    "    ## initializing model and running the optimizationportion_parent\n",
    "    try:\n",
    "        metainfo = {}\n",
    "        metainfo['dflat'] = dflat\n",
    "        metainfo['dcon'] = dcon\n",
    "        metainfo['concepts'] = concepts                            \n",
    "        model = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dflat, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=w_threshold, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo\n",
    "        )\n",
    "        W_notears, res = nonlinear_concept.notears_nonlinear(\n",
    "            model, Xflat, lambda1=val_lambda, lambda2=val_lambda,\n",
    "            h_tol=1e-8, rho_max=1e+18\n",
    "        ) ## lambda1=0.01, lambda2=0.01, h_tol=1e-8, rho_max=1e+16\n",
    "        # assert ut.is_dag(W_notears)\n",
    "        np.savetxt('outputs/W_con_' + str(should_std) + str(val_lambda) + str(w_threshold) + '.csv', W_notears, delimiter=',')\n",
    "        print('W_con', W_notears)\n",
    "        #\n",
    "        #\n",
    "    except Exception as e:\n",
    "        print('========================================', e)\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    ## initializing model and running the optimizaportion_parenttion\n",
    "    def conv_flat_to_con(A, concepts):\n",
    "\n",
    "        ##\n",
    "        A = np.abs(A) ## in the optimization this works on square matrix, so there we don't need to abs it\n",
    "        dflat = sum(concepts)\n",
    "        dcon = len(concepts)\n",
    "        Arow = np.zeros((dcon,dflat))\n",
    "        Ad = np.zeros((dcon,dcon))\n",
    "        end_concept = np.cumsum(concepts)\n",
    "\n",
    "        ##\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Arow[i,:] = (A[start_i:end_i,:].sum(axis=0))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Ad[:,i] = (Arow[:,start_i:end_i].sum(axis=1))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "\n",
    "        ##\n",
    "        new_adj_mat = np.zeros((dcon,dcon))\n",
    "        for i in range(dcon):\n",
    "            for j in range(dcon):\n",
    "                if Ad[i][j] != 0:\n",
    "                    new_adj_mat[i][j] = 1\n",
    "\n",
    "        return new_adj_mat\n",
    "\n",
    "    try:\n",
    "        model3 = nonlinear_old.NotearsMLP(dims=[dflat, 10, 1], bias=True)\n",
    "        W_notears3 = nonlinear_old.notears_nonlinear(\n",
    "            model3, Xflat, lambda1=val_lambda, lambda2=val_lambda, w_threshold=w_threshold,\n",
    "            h_tol=1e-8, rho_max=1e+18\n",
    "        ) ## lambda1=0.01, lambda2=0.01, w_threshold=0.3, h_tol=1e-8, rho_max=1e+16\n",
    "        W_notears3 = conv_flat_to_con(W_notears3, concepts)\n",
    "        # assert ut.is_dag(W_notears3)\n",
    "        np.savetxt('outputs/W_flat_' + str(should_std) + str(val_lambda) + str(w_threshold) + '.csv', W_notears3, delimiter=',')\n",
    "        print('W_flat', W_notears3)        \n",
    "        #\n",
    "        #\n",
    "    except Exception as e:\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 23:35:36,376\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(get_result pid=12160)\u001b[0m [1, 17, 5, 1, 1] 5 25\n",
      "\u001b[2m\u001b[36m(get_result pid=12160)\u001b[0m -----iteration no:  0\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=56) ## detects automatically: num_cpus=64\n",
    "    \n",
    "\n",
    "    list_option = [\n",
    "#         (False, 0.01, 0.5),        \n",
    "#         (False, 0.01, 0.4),\n",
    "#         (False, 0.01, 0.3),\n",
    "#         (False, 0.01, 0.2),\n",
    "#         (False, 0.01, 0.1),        \n",
    "#         (False, 0.005, 0.5),        \n",
    "#         (False, 0.005, 0.4),\n",
    "#         (False, 0.005, 0.3),\n",
    "#         (False, 0.005, 0.2),\n",
    "#         (False, 0.005, 0.1),        \n",
    "#         (False, 0.001, 0.5),        \n",
    "#         (False, 0.001, 0.4),\n",
    "#         (False, 0.001, 0.3),\n",
    "#         (False, 0.001, 0.2),\n",
    "#         (False, 0.001, 0.1),        \n",
    "\n",
    "#         (True, 0.01, 0.5),        \n",
    "#         (True, 0.01, 0.4),\n",
    "#         (True, 0.01, 0.3),\n",
    "#         (True, 0.01, 0.2),\n",
    "#         (True, 0.01, 0.1),        \n",
    "#         (True, 0.005, 0.5),        \n",
    "#         (True, 0.005, 0.4),\n",
    "#         (True, 0.005, 0.3),\n",
    "#         (True, 0.005, 0.2),\n",
    "#         (True, 0.005, 0.1),        \n",
    "#         (True, 0.001, 0.5),        \n",
    "#         (True, 0.001, 0.4),\n",
    "#         (True, 0.001, 0.3),\n",
    "#         (True, 0.001, 0.2),\n",
    "        (True, 0.001, 0.1),        \n",
    "        \n",
    "    ]\n",
    "    df_x = pd.read_csv('datasets/movie_processed_1.csv')\n",
    "    ## bud, dir, cast, genre, imdb, rev   \n",
    "    df_x = df_x[[\n",
    "        'budget',   \n",
    "        'c1','c2', 'c3','c4', 'c5','c6', 'c7','c8', 'c9','c10', 'c11', 'c12', 'c13', 'c14','c15', 'c16','c17',    \n",
    "        'g1','g2', 'g3','g4', 'g5',\n",
    "        'imdb_user_rating',\n",
    "        'revenue'        \n",
    "    ]]\n",
    "        \n",
    "    \n",
    "    list_result_id = []\n",
    "    for opt in list_option:\n",
    "        result_id = get_result.remote(\n",
    "            df_x, opt\n",
    "        )\n",
    "        list_result_id.append(result_id)\n",
    "    list_result = ray.get(list_result_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
