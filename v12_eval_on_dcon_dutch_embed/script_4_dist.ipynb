{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import graphviz\n",
    "import notears.utils as ut\n",
    "from notears import nonlinear_concept, nonlinear_old\n",
    "import igraph as ig\n",
    "# import lingam\n",
    "# from lingam.utils import make_prior_knowledge, make_dot\n",
    "import ray\n",
    "import pickle as pk\n",
    "from scipy.special import expit as sigmoid\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.23.3', '1.4.2']\n"
     ]
    }
   ],
   "source": [
    "## environmental setup\n",
    "\n",
    "print([np.__version__, pd.__version__])\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change_0 ===========================================================================================\n",
    "\n",
    "# df_1 = pd.read_csv('datasets/adult_data.csv', header=None)\n",
    "# df_2 = pd.read_csv('datasets/adult_test.csv', header=None)\n",
    "# df_3 = pd.concat([df_1, df_2])\n",
    "# for col in list(df_3.columns):\n",
    "#     df_3 = df_3[df_3[col] != ' ?'] \n",
    "\n",
    "# list_col = list(df_3.columns)\n",
    "# list_remove = [2,4,10,11] ## fnlwgt, education-num, capital-gain, capital-loss\n",
    "# list_col_valid = []\n",
    "# for col in list_col:\n",
    "#     if col not in list_remove:\n",
    "#         list_col_valid.append(col)\n",
    "# print(list_col_valid)\n",
    "\n",
    "# df_4 = df_3[list_col_valid]\n",
    "# df_4 = df_4.replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'})\n",
    "# print(df_4.shape)\n",
    "\n",
    "# df_4.to_csv('datasets/adult_processed.csv', index=False, header=None) \n",
    "\n",
    "# change_0 ==========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60420, 12)\n",
      "(12, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>household_position</th>\n",
       "      <th>household_size</th>\n",
       "      <th>prev_residence</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>country_birth</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>economic_status</th>\n",
       "      <th>cur_eco_activity</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  age   household_position  household_size  prev_residence  \\\n",
       "sex     0     0                   1               0               0   \n",
       "age     0     0                   1               1               1   \n",
       "\n",
       "      citizenship  country_birth  edu_level  economic_status  \\\n",
       "sex             0              0          1                1   \n",
       "age             0              0          1                1   \n",
       "\n",
       "      cur_eco_activity  marital_status  occupation  \n",
       "sex                  1               1           1  \n",
       "age                  1               1           1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change_1 ============================================================================================\n",
    "## data and causal graph\n",
    "\n",
    "df_x = pd.read_csv('datasets/dutch.csv', header=None)\n",
    "df_cg = pd.read_excel(open('datasets/dutch.xlsx', 'rb'), index_col=0)\n",
    "\n",
    "print(df_x.shape)\n",
    "df_x.head(2)\n",
    "\n",
    "print(df_cg.shape)\n",
    "df_cg.head(2)\n",
    "## change_1 ============================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concepts = []\n",
    "\n",
    "# list_index_continuous = [] ## \n",
    "# Xcon, B_true = df_x.values, df_cg.values\n",
    "# Xflat = None\n",
    "# for i in range(Xcon.shape[1]):\n",
    "#     Xlocal = Xcon[:, i].reshape(-1, 1)    \n",
    "#     if i in list_index_continuous:\n",
    "#         Xlocalflat = Xlocal\n",
    "#     else:\n",
    "#         Xlocalflat = LabelEncoder().fit_transform(Xlocal).reshape(-1,1)\n",
    "        \n",
    "#         # Define the size of the vocabulary and the size of the embedding vectors\n",
    "#         vocab_size = np.max(Xlocalflat) + 1\n",
    "#         embedding_dim = math.floor(math.log2(vocab_size) + 1)\n",
    "    \n",
    "#         # Create an instance of the Embedding class\n",
    "#         embedding_layer = nn.Embedding(vocab_size, embedding_dim)    \n",
    "#         embedding_vectors = embedding_layer(torch.tensor(Xlocalflat))\n",
    "#         Xlocalflat = embedding_vectors.reshape(-1, embedding_dim).detach().numpy()    \n",
    "    \n",
    "#     concepts.append(Xlocalflat.shape[1])\n",
    "#     if Xflat is None:\n",
    "#         Xflat = Xlocalflat\n",
    "#     else:\n",
    "#         Xflat = np.hstack((Xflat, Xlocalflat))\n",
    "#     print(i, Xlocal.shape, Xlocalflat.shape, vocab_size)\n",
    "# print()\n",
    "# print(concepts, sum(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### experiment\n",
    "\n",
    "## functions and classes \n",
    "@ray.remote(num_returns=1)\n",
    "def get_result(data_x, data_cg, should_std, trial_no):\n",
    "        \n",
    "    ## 1\n",
    "    np.random.seed(123+trial_no) \n",
    "    ut.set_random_seed(123+trial_no) \n",
    "\n",
    "    ## 2\n",
    "    ## change_2 ==========================================================================================    \n",
    "\n",
    "    ## all are categorical\n",
    "    ## 0. sex: -\n",
    "    ## 1. age: -\n",
    "    ## 2. household_position: -\n",
    "    ## 3. household_size: -\n",
    "    ## 4. prev_residence_place: -\n",
    "    ## 5. citizenship: -\n",
    "    ## 6. country_birth: -\n",
    "    ## 7. edu_level: -\n",
    "    ## 8. economic_status: -\n",
    "    ## 9. cur_eco_activity: -\n",
    "    ## 10. Marital_status: -     \n",
    "    ## 11. occupation: -\n",
    "\n",
    "    concepts = []    \n",
    "    list_index_continuous = [] ## \n",
    "    Xcon, B_true = df_x.values, df_cg.values\n",
    "    Xflat = None\n",
    "    for i in range(Xcon.shape[1]):\n",
    "        Xlocal = Xcon[:, i].reshape(-1, 1)    \n",
    "        if i in list_index_continuous:\n",
    "            Xlocalflat = Xlocal\n",
    "        else:\n",
    "            Xlocalflat = LabelEncoder().fit_transform(Xlocal.ravel()).reshape(-1,1)\n",
    "            # Define the size of the vocabulary and the size of the embedding vectors\n",
    "            vocab_size = np.max(Xlocalflat) + 1\n",
    "            embedding_dim = math.floor(math.log2(vocab_size) + 1)\n",
    "            # Create an instance of the Embedding class\n",
    "            embedding_layer = nn.Embedding(vocab_size, embedding_dim)    \n",
    "            embedding_vectors = embedding_layer(torch.tensor(Xlocalflat))\n",
    "            Xlocalflat = embedding_vectors.reshape(-1, embedding_dim).detach().numpy()    \n",
    "\n",
    "        concepts.append(Xlocalflat.shape[1])\n",
    "        if Xflat is None:\n",
    "            Xflat = Xlocalflat\n",
    "        else:\n",
    "            Xflat = np.hstack((Xflat, Xlocalflat))    \n",
    "    \n",
    "    n, d = Xcon.shape\n",
    "    s0 = sum(sum(B_true))   \n",
    "    print('concepts ======> ', concepts)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dcon, dflat = len(concepts), sum(concepts)\n",
    "    print(n, d, s0, concepts, dcon, dflat, Xcon.shape, B_true.shape, Xflat.shape)\n",
    "\n",
    "    ## 3\n",
    "    if should_std:\n",
    "        # scalerCon = StandardScaler().fit(Xcon)\n",
    "        # Xcon = scalerCon.transform(Xcon) ## this includes string values, so can't transform    \n",
    "        scalerFlat = StandardScaler().fit(Xflat)\n",
    "        Xflat = scalerFlat.transform(Xflat)    \n",
    "    # Xcon, Xflat = Xcon.astype('float32'), Xflat.astype('float32')\n",
    "    Xflat = Xflat.astype('float32')\n",
    "    ## change_2 ==========================================================================================    \n",
    "        \n",
    "\n",
    "    ## 4\n",
    "    mask = np.ones((dcon, dcon)) * np.nan\n",
    "    print(concepts, dcon, dflat)\n",
    "    assert len(concepts) == dcon \n",
    "    assert sum(concepts) == dflat\n",
    "    assert Xcon.shape[1] == dcon        \n",
    "    assert Xflat.shape[1] == dflat    \n",
    "\n",
    "    ## initializing model and running the optimizationportion_parent\n",
    "    try:\n",
    "        metainfo = {}\n",
    "        metainfo['dflat'] = dflat\n",
    "        metainfo['dcon'] = dcon\n",
    "        metainfo['concepts'] = concepts                            \n",
    "        model = nonlinear_concept.NotearsMLP(\n",
    "            dims=[dflat, 10, 1], bias=True,\n",
    "            mask=mask, w_threshold=0.2, learned_model=None, ## w_threshold=0.3\n",
    "            metainfo=metainfo\n",
    "        )\n",
    "        W_notears, res = nonlinear_concept.notears_nonlinear(\n",
    "            model, Xflat, lambda1=0.001, lambda2=0.001,\n",
    "            h_tol=1e-4, rho_max=1e+8\n",
    "        ) ## lambda1=0.01, lambda2=0.01, h_tol=1e-8, rho_max=1e+16\n",
    "        # assert ut.is_dag(W_notears)\n",
    "        # np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_notears != 0)\n",
    "        print('nCon: ', acc)\n",
    "        print(W_notears)\n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, nCon ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            should_std, trial_no, \n",
    "            acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']\n",
    "        )\n",
    "        file1.writelines(s1)\n",
    "        file1.close()    \n",
    "        #\n",
    "    except Exception as e:\n",
    "        print('========================================', e)\n",
    "        acc = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    ## initializing model and running the optimizaportion_parenttion\n",
    "    def conv_flat_to_con(A, concepts):\n",
    "\n",
    "        ##\n",
    "        A = np.abs(A) ## in the optimization this works on square matrix, so there we don't need to abs it\n",
    "        dflat = sum(concepts)\n",
    "        dcon = len(concepts)\n",
    "        Arow = np.zeros((dcon,dflat))\n",
    "        Ad = np.zeros((dcon,dcon))\n",
    "        end_concept = np.cumsum(concepts)\n",
    "\n",
    "        ##\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Arow[i,:] = (A[start_i:end_i,:].sum(axis=0))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "        start_i = 0\n",
    "        for i in range(dcon):\n",
    "            end_i = end_concept[i]\n",
    "            Ad[:,i] = (Arow[:,start_i:end_i].sum(axis=1))/(end_i-start_i)\n",
    "            start_i = end_i\n",
    "\n",
    "        ##\n",
    "        new_adj_mat = np.zeros((dcon,dcon))\n",
    "        for i in range(dcon):\n",
    "            for j in range(dcon):\n",
    "                if Ad[i][j] != 0:\n",
    "                    new_adj_mat[i][j] = 1\n",
    "\n",
    "        return new_adj_mat\n",
    "\n",
    "    try:\n",
    "        model3 = nonlinear_old.NotearsMLP(dims=[dflat, 10, 1], bias=True)\n",
    "        W_notears3 = nonlinear_old.notears_nonlinear(\n",
    "            model3, Xflat, lambda1=0.001, lambda2=0.001, w_threshold=0.2,\n",
    "            h_tol=1e-4, rho_max=1e+8\n",
    "        ) ## lambda1=0.01, lambda2=0.01, w_threshold=0.3, h_tol=1e-8, rho_max=1e+16\n",
    "        W_notears3 = conv_flat_to_con(W_notears3, concepts)\n",
    "        # assert ut.is_dag(W_notears3)\n",
    "        # np.savetxt('outputs/W_notears3.csv', W_notears3, delimiter=',')\n",
    "        acc3 = ut.count_accuracy(B_true, W_notears3 != 0)\n",
    "        print('nRegFlat', acc3)\n",
    "        print(W_notears3)        \n",
    "        #\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"{}, {}, nRegFlat ==> {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n",
    "            should_std, trial_no, \n",
    "            acc3['fdr'], acc3['tpr'], acc3['fpr'], acc3['shd'], acc3['nnz']\n",
    "        )                            \n",
    "        file1.writelines(s1)\n",
    "        file1.close()\n",
    "        #\n",
    "    except Exception as e:\n",
    "        acc3 = {\n",
    "            'fdr': '-',\n",
    "            'tpr': '-',\n",
    "            'fpr': '-',\n",
    "            'shd': '-',\n",
    "            'nnz': '-'\n",
    "        }\n",
    "        file1 = open('logger.log', 'a+')  \n",
    "        s1 = \"Error ==> {}\\n\".format(e)\n",
    "        file1.writelines(s1)\n",
    "        file1.close()                    \n",
    "\n",
    "\n",
    "    #################################################\n",
    " \n",
    "    \n",
    "    return [\n",
    "        (acc['fdr'], acc['tpr'], acc['fpr'], acc['shd'], acc['nnz']), \n",
    "        (acc3['fdr'], acc3['tpr'], acc3['fpr'], acc3['shd'], acc3['nnz']),        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 19:44:53,566\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(get_result pid=1976)\u001b[0m concepts ======>  [2, 4, 4, 3, 2, 2, 2, 3, 2, 4, 3, 2]\n",
      "\u001b[2m\u001b[36m(get_result pid=1976)\u001b[0m 60420 12 44 [2, 4, 4, 3, 2, 2, 2, 3, 2, 4, 3, 2] 12 33 (60420, 12) (12, 12) (60420, 33)\n",
      "\u001b[2m\u001b[36m(get_result pid=1976)\u001b[0m [2, 4, 4, 3, 2, 2, 2, 3, 2, 4, 3, 2] 12 33\n",
      "\u001b[2m\u001b[36m(get_result pid=1976)\u001b[0m -----iteration no:  0\n",
      "\u001b[2m\u001b[36m(get_result pid=9352)\u001b[0m concepts ======>  [2, 4, 4, 3, 2, 2, 2, 3, 2, 4, 3, 2]\n",
      "\u001b[2m\u001b[36m(get_result pid=9352)\u001b[0m 60420 12 44 [2, 4, 4, 3, 2, 2, 2, 3, 2, 4, 3, 2] 12 33 (60420, 12) (12, 12) (60420, 33)\n",
      "\u001b[2m\u001b[36m(get_result pid=9352)\u001b[0m [2, 4, 4, 3, 2, 2, 2, 3, 2, 4, 3, 2] 12 33\n",
      "\u001b[2m\u001b[36m(get_result pid=9352)\u001b[0m -----iteration no:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    ## variables\n",
    "    list_should_std = [False, True]\n",
    "    n_trials = 2\n",
    "    \n",
    "    ## variables            \n",
    "    ray.shutdown()\n",
    "    ray.init(ignore_reinit_error=True, num_cpus=56) ## detects automatically: num_cpus=64\n",
    "\n",
    "    ## experiments\n",
    "    for should_std in list_should_std:\n",
    "        list_result_id = []\n",
    "        for trial_no in range(n_trials):\n",
    "            result_id = get_result.remote(\n",
    "                df_x, df_cg, should_std, trial_no\n",
    "            )\n",
    "            list_result_id.append(result_id)\n",
    "        list_result = ray.get(list_result_id)\n",
    "\n",
    "        d_result = {}\n",
    "        for trial_no in range(n_trials):\n",
    "            d_result[(should_std, trial_no, 'nCon')] = list_result[trial_no][0]\n",
    "            d_result[(should_std, trial_no, 'nRegFlat')] = list_result[trial_no][1]                                \n",
    "\n",
    "        with open(\n",
    "            'datasets/d_result_' + str(should_std) + '.pickle', 'wb'\n",
    "        ) as handle: \n",
    "            pk.dump(d_result, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
