{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## script_5_concept_embedding.ipynb\n",
    "## building up a process to use concept-singleton, singleton-concept, and concept-concept interaction\n",
    "## using all numerical\n",
    "## ==> can be cyclic: 1) check assert DAG 2) in utils.py look for DAG check\n",
    "## ==> i dont think its (problem of low perform in case of concepts) related to choosing w_threshold, \n",
    "##     rather with the data generating process, check for concepts_fake with dimensions [1, 1, 1, ..., 1]\n",
    "##     concepts(W_notears) and concepts_fake (W_notears2)\n",
    "## ==> made the CustomNN simpler, also using additive noise model to generate data\n",
    "## ==> normalized adjacency matrix for concept\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import notears.utils as ut\n",
    "import pandas as pd\n",
    "from notears import nonlinear\n",
    "import igraph as ig\n",
    "import torch.nn as nn\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables\n",
    "\n",
    "concept_dim_limit=10\n",
    "param_scale = 4\n",
    "hidden_unit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions and classes\n",
    "\n",
    "def get_transformed_data(dim_input, dim_output, data_input):\n",
    "\n",
    "    class CustomNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CustomNN, self).__init__()\n",
    "            self.nn_reg = nn.Sequential(\n",
    "                nn.Linear(dim_input, hidden_unit),\n",
    "                nn.Sigmoid(),\n",
    "                \n",
    "                nn.Linear(hidden_unit, dim_output),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.nn_reg(x)\n",
    "            return output\n",
    "        \n",
    "    model = CustomNN()\n",
    "    data_output = model(data_input)\n",
    "    return data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up environment params\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3)\n",
    "ut.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## (1) generate a causal graph at random as you have done already (eg. x1->x2) but this time it will represent relations between concepts, \n",
    "\n",
    "n, dcon, s0, graph_type, sem_type = 200, 5, 9, 'ER', 'mlp'\n",
    "B_true = ut.simulate_dag(dcon, s0, graph_type)\n",
    "np.savetxt('datasets/W_true.csv', B_true, delimiter=',')\n",
    "# X = ut.simulate_nonlinear_sem(B_true, n, sem_type)\n",
    "# np.savetxt('datasets/X.csv', X, delimiter=',')\n",
    "print(B_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 10, 4, 3]\n",
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "## (2) randomly decide the embedding size of your concepts (eg. dim(x1)=3, dim(x2)=5). \n",
    "\n",
    "concepts = torch.randint(1, concept_dim_limit+1, (dcon,)) \n",
    "concepts = [int(i) for i in concepts]\n",
    "print(concepts)\n",
    "\n",
    "concepts_fake = torch.randint(1, 2, (dcon,)) \n",
    "concepts_fake = [int(i) for i in concepts_fake]\n",
    "print(concepts_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) generate a list of neural networks for each effect concept (eg. nn_x2 (input=3, output=5, weights=random), \n",
    "## (4) generate data for x1 = randn(dim=3) for x2 = nn_x2(x1) + eps*rand(dim=5)\n",
    "\n",
    "def get_generated_data(con, B_true, dcon, n):\n",
    "    dflat = sum(con)\n",
    "    G = ig.Graph.Adjacency(B_true.tolist())\n",
    "    ordered_vertices = G.topological_sorting()  \n",
    "    assert len(ordered_vertices) == dcon\n",
    "\n",
    "    dict_new_x = {}\n",
    "    for v_index in ordered_vertices:\n",
    "        col = B_true[:, v_index]\n",
    "        col_sum = np.sum(col, axis=0)\n",
    "        if col_sum == 0:\n",
    "            portion_parent = 0\n",
    "        else:\n",
    "            dim_output = con[v_index]\n",
    "            dim_input = 0\n",
    "            data_input = None\n",
    "            i=0\n",
    "            for row in col:\n",
    "                if row == 1:\n",
    "                    dim_input += con[i]\n",
    "                    if data_input is None:\n",
    "                        data_input = dict_new_x[i]\n",
    "                    else:\n",
    "                        data_input = torch.cat([data_input, dict_new_x[i]], dim=1) \n",
    "                i+=1\n",
    "\n",
    "            data_output = get_transformed_data(dim_input, dim_output, data_input)\n",
    "            portion_parent = data_output.detach()\n",
    "\n",
    "        portion_noise = torch.randn(n, con[v_index])\n",
    "        if col_sum == 0:\n",
    "            new_x = portion_noise\n",
    "        else:\n",
    "            new_x = portion_parent + portion_noise\n",
    "        dict_new_x[v_index] = param_scale * new_x\n",
    "\n",
    "    Xf = dict_new_x[0]\n",
    "    for i in range(1, dcon):\n",
    "        Xf = np.hstack((Xf, dict_new_x[i]))\n",
    "    return Xf\n",
    "\n",
    "Xf = get_generated_data(concepts, B_true, dcon, n)\n",
    "Xf_fake = get_generated_data(concepts_fake, B_true, dcon, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 23)\n",
      "[[ 1.352  4.414 -3.455  2.089  8.818  6.098 -5.84  -0.488  6.432 -3.375\n",
      "   3.737 -3.957 12.039 -4.294 -2.784 -2.027 -4.482  1.182 -5.938  0.493\n",
      "   4.919 -2.257  8.217]\n",
      " [-2.362 -6.079 -2.877  6.747  0.736 -1.661 -2.521  1.046  1.02  -0.583\n",
      "   0.196 10.539  5.156  2.429 -0.486 -0.069  1.713 -5.668  2.336 -3.646\n",
      "  -1.075  4.082 -0.94 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Xf.shape)\n",
    "print(Xf[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5)\n",
      "[[-8.978  1.068 -2.203  7.569  1.623]\n",
      " [-0.37   1.554 -0.171  3.762 -3.282]]\n"
     ]
    }
   ],
   "source": [
    "print(Xf_fake.shape)\n",
    "print(Xf_fake[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 23)\n",
      "15396.716116979487\n",
      "\n",
      "(200, 5)\n",
      "3189.324408157695\n"
     ]
    }
   ],
   "source": [
    "## properties of X, Xf\n",
    "\n",
    "print(Xf.shape)\n",
    "print(sum(sum(np.abs(Xf))))\n",
    "print()\n",
    "print(Xf_fake.shape)\n",
    "print(sum(sum(np.abs(Xf_fake))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## induced bias\n",
    "\n",
    "mask = np.ones((dcon, dcon)) * np.nan\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 23)\n",
      "200 {'dflat': 23, 'dcon': 5, 'concepts': [5, 1, 10, 4, 3]}\n"
     ]
    }
   ],
   "source": [
    "## forming metainfo related to concepts\n",
    "\n",
    "\n",
    "print(Xf.shape)\n",
    "n, dflat = Xf.shape[0], Xf.shape[1]\n",
    "\n",
    "assert len(concepts) == dcon \n",
    "assert sum(concepts) == dflat\n",
    "\n",
    "metainfo = {}\n",
    "metainfo['dflat'] = dflat\n",
    "metainfo['dcon'] = dcon\n",
    "metainfo['concepts'] = concepts\n",
    "\n",
    "print(n, metainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nonlinear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25ab0dc4fb24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## initializing model and running the optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m model = nonlinear.NotearsMLP(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdflat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearned_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nonlinear' is not defined"
     ]
    }
   ],
   "source": [
    "## initializing model and running the optimization\n",
    "\n",
    "model = nonlinear.NotearsMLP(\n",
    "    dims=[dflat, 10, 1], bias=True,\n",
    "    mask=mask, w_threshold=0.3, learned_model=None,\n",
    "    metainfo=metainfo\n",
    ")\n",
    "\n",
    "W_notears, res = nonlinear.notears_nonlinear(model, Xf, lambda1=0.01, lambda2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fdr': 0.3, 'tpr': 0.7777777777777778, 'fpr': 3.0, 'shd': 3, 'nnz': 10}\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[0.    0.    2.107 0.    1.742]\n",
      " [1.962 0.    1.679 3.145 1.565]\n",
      " [0.    0.    0.    0.    0.886]\n",
      " [1.722 0.    1.685 0.    1.699]\n",
      " [0.    0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "## performance evaluation\n",
    "\n",
    "# assert ut.is_dag(W_notears)\n",
    "np.savetxt('outputs/W_notears.csv', W_notears, delimiter=',')\n",
    "acc = ut.count_accuracy(B_true, W_notears != 0)\n",
    "print(acc)\n",
    "print(B_true)\n",
    "print(W_notears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----iteration no:  0\n",
      "-----iteration no:  1\n",
      "-----iteration no:  2\n",
      "-----iteration no:  3\n",
      "-----iteration no:  4\n",
      "-----iteration no:  5\n",
      "-----iteration no:  6\n",
      "-----iteration no:  7\n",
      "-----iteration no:  8\n",
      "-----iteration no:  9\n",
      "-----iteration no:  10\n",
      "-----iteration no:  11\n"
     ]
    }
   ],
   "source": [
    "## initializing model and running the optimization\n",
    "\n",
    "metainfo2 = {}\n",
    "metainfo2['dflat'] = dcon\n",
    "metainfo2['dcon'] = dcon\n",
    "metainfo2['concepts'] = [1 for i in range(dcon)]\n",
    "\n",
    "model2 = nonlinear.NotearsMLP(\n",
    "    dims=[dcon, 10, 1], bias=True,\n",
    "    mask=mask, w_threshold=0.3, learned_model=None,\n",
    "    metainfo=metainfo2\n",
    ")\n",
    "\n",
    "W_notears2, res2 = nonlinear.notears_nonlinear(model2, Xf_fake, lambda1=0.01, lambda2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fdr': 0.5, 'tpr': 0.5555555555555556, 'fpr': 5.0, 'shd': 5, 'nnz': 10}\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[0.    4.423 2.854 0.    4.132]\n",
      " [0.    0.    3.313 0.    3.531]\n",
      " [0.    0.    0.    0.    5.135]\n",
      " [2.831 3.35  2.524 0.    2.331]\n",
      " [0.    0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "## performance evaluation\n",
    "\n",
    "# assert ut.is_dag(W_notears2)\n",
    "np.savetxt('outputs/W_notears2.csv', W_notears2, delimiter=',')\n",
    "acc2 = ut.count_accuracy(B_true, W_notears2 != 0)\n",
    "print(acc2)\n",
    "print(B_true)\n",
    "print(W_notears2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
